---
title: "NCAA Tournament Contest"
description: |
  Scraping Data
output:
  radix::radix_article:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# A Brief Review

Last time, we scraped data coming from a few different sources: tables, things that look like tables, and numbers clearly coming from Dante's 5th circle. The bulk of our work was done using the common `read_html` to `html_table` or `html_nodes`, we did see that we need to utilize a little bit of XML for some of the more complicated affairs.  

While our focus last time was mainly getting the data in, we are going to focus our attention now on cleaning the data and hoping to join it together.

# Package Install

```{r, eval = FALSE}
install.packages(c("stringr", "fuzzyjoin"))
```


# Basic Data Manipulation With Regular Expressions

```{r}
library(dplyr)

library(fuzzyjoin)

library(stringr)

library(tidyr)

load("basketballData.RData")
```

There will be times were very simple operations need to be done for creating a table -- the oneManData is such a time.

```{r}
oneManData
```

When looking at that vector, we see that we have the rankings, a period, a space, and a team name. We can't just split on a space or even a ". ", so we need to get a little bit crafty with our regular expressions.

```{r}
oneManDataSplit <- strsplit(oneManData, "(?<=[0-9])\\.\\W", perl = TRUE)

oneManDataSplit
```

That bit of gibberish there is a *lookaround* (specifically, a positive lookbehind). In words, we are saying to find a period (`\\.`), followed directly by any non-word characters (`\\W`), that is directly preceeded by a number.

Since we know how to best split our data, let's split those characters into two separate pieces of data:

```{r}
oneManData <- data.frame(all = oneManData) %>% 
  separate(., col = all, into = c("oneManRank", "Team"), sep = "(?<=[0-9])\\.\\W") %>% 
  na.omit() %>% 
  mutate(oneManRank = as.numeric(oneManRank)) %>% 
  arrange(oneManRank)

rmarkdown::paged_table(oneManData)
```

That separate function is handy for performing such work. We define the new column names and the separation, and the function takes care of everything else. 

If we take another look at our data from Ken Pomeroy, we see that we might have an interesting issues to apply some of our newly-found regular expression skills to.

```{r}
rmarkdown::paged_table(kenPomData)
```

We need to do something about those columns names:

```{r}
oldNames <- names(kenPomData)

newNames <- kenPomData[1, ]

newNames <- unlist(ifelse(oldNames != "", paste(newNames, oldNames, sep = "_"), newNames))

newNames <- gsub("\\s|-", "_", newNames)

newNames <- ifelse(duplicated(newNames), paste(newNames, "rank", sep = "_"), newNames)

names(kenPomData) <- newNames

kenPomData <- kenPomData %>% 
  filter(Rk != "" & Rk != "Rk")

rmarkdown::paged_table(kenPomData)
```


Finally, we can clean up the variables and convert them appropriately:

```{r}
kenPomData <- kenPomData %>% 
  separate(., W_L, into = c("Wins", "Losses")) %>%
  mutate_at(., .vars = vars(Rk, Wins:AdjEM_NCSOS_rank), as.numeric) %>% 
  mutate(Win_Percentage = Wins / (Wins + Losses))

rmarkdown::paged_table(kenPomData)
```


# String Processing

Let's begin by returning to one of our first tables that we pulled in from the web: the nittyGritty table

```{r}
head(nittyGrittyData)
```

For the sake of getting the data in shape, we need to drop those rows without names:

```{r}
dropCols <- which(names(nittyGrittyData) == "")

nittyGrittyData <- nittyGrittyData[, -c(dropCols)]
```

While things looked good with a cursory glance, we see some issues straight away. The major issue is that we have strings smashed together. Fortunately, this is not too big of a problem:

```{r}
nittyGrittyData$Team <- gsub("\\(|\\)", "", nittyGrittyData$Team)

nittyGrittyData <- nittyGrittyData %>% 
  filter(Team != "Team") %>% 
  mutate(confRecord = str_extract(Team, "[0-9]+-[0-9]+"), 
         Team = gsub("[0-9]+-[0-9]+", "", Team)) %>% 
  separate(., col = Team, into = c("Team", "Conference"), 
           sep = "(?<=([a-z])([A-Z]))", remove = FALSE) %>% 
  mutate(Conference = ifelse(is.na(Conference) == FALSE, 
                             paste(str_extract(Team, "[A-Z]$"), Conference, sep = ""), 
                             NA), 
         Conference = str_squish(Conference))

conferenceList <- sort(unique(nittyGrittyData$Conference))

nittyGrittyData <- nittyGrittyData %>% 
  mutate(Team = ifelse(is.na(Conference), Team, 
                       gsub("[A-Z]$", "", Team)),
         Conference = ifelse(is.na(Conference), 
                             str_extract(Team, paste(conferenceList, collapse = "|")), 
                             Conference), 
         Team = gsub(paste(conferenceList, collapse = "|"), "", Team))

rmarkdown::paged_table(nittyGrittyData)
```

# Joining

That was some work, but the frustration is just beginning. Ideally, we want to be able to pull all of our data together into one big data frame. In theory, this should be remarkably easy; in practice, it is never simple.

We will want to join on team names, so let's see what we have:

```{r}
teamNames <- as.data.frame(do.call("cbind", 
                                  list(sort(hoopMathData$Team), 
                                       sort(ncaaData$School), 
                                       sort(masseyComparison$Team), 
                                       sort(haslamData$t), 
                                       sort(kenPomData$Team), 
                                       sort(oneManData$Team))))
```

Clearly the good people at hoopmath don't hate the world, so they have maintained some consistency with the NCAA. This will actually be an easy join!

```{r}
almostCompleteData <- left_join(hoopMathData, ncaaData, by = c("Team" = "School"))

rmarkdown::paged_table(almostCompleteData)
```


```{r}
stringNormalizer <- function(x) {
  joiningName = gsub("\\.|\\(|\\)", "", x, perl = TRUE) 
  joiningName = gsub("-", " ", joiningName, perl = TRUE)
  joiningName = gsub("St$", "State", joiningName, perl = TRUE)
  joiningName = gsub("St(?!\\w)", "Saint", joiningName, perl = TRUE)
  joiningName = gsub("(?<!\\w)No(?!\\w)|(?<!\\w)N(?!\\w)", "North", joiningName, perl = TRUE)
  joiningName = gsub("(?<!\\w)So(?!\\w)|(?<!\\w)S(?!\\w)", "South", joiningName, perl = TRUE)
  joiningName = gsub("(?<!\\w)E(?!\\w)", "East", joiningName, perl = TRUE)
  joiningName = gsub("(?<!\\w)W(?!\\w)", "West", joiningName, perl = TRUE)
  joiningName = gsub("(?<!\\w)C(?!\\w)", "Central", joiningName, perl = TRUE)
  return(joiningName)
}

almostCompleteData$joiningName = stringNormalizer(almostCompleteData$Team)
```

We won't even demo the kind of problems a normal join would create if we try to join the massey data in with our nice data.

```{r}
masseyComparison$joiningName <- stringNormalizer(masseyComparison$Team)

masseyTest <- stringdist_left_join(almostCompleteData, masseyComparison, by = "joiningName", 
                            method = "jw", max_dist = .1, distance_col = "distance")
```


Check out how we did:

```{r, eval = FALSE}
masseyTest %>% 
  select(Team.x, Team.y, joiningName.x, joiningName.y, distance) %>% 
  View()
```

This should give us some thoughts for additional cleaning.

Now, let's see how something else might join:

```{r}
kenPomData$joiningName <- stringNormalizer(kenPomData$Team)

kenPomTest <- stringdist_left_join(almostCompleteData, kenPomData, by = "joiningName", 
                            method = "jw", max_dist = .1, distance_col = "distance")
```

And check:

```{r, eval = FALSE}
kenPomTest %>% 
  select(Team.x, Team.y, joiningName.x, joiningName.y, distance) %>% 
  View()
```
